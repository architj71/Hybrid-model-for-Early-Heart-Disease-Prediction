{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf79mdMlWZp9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTGNevaeWqvM",
    "outputId": "ba2f86e2-72e4-49a4-9539-cae0459cd55a"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"cardio_train.csv\", delimiter=\";\")\n",
    "\n",
    "# Convert target to binary (0: No HD, 1: HD)\n",
    "df['cardio'] = df['cardio'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_imputed.drop('cardio', axis=1)\n",
    "y = df_imputed['cardio']\n",
    "\n",
    "# Balance classes with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# Normalize data\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X_res)\n",
    "\n",
    "# Calculate feature importance via Extra Trees\n",
    "etc = ExtraTreesClassifier(n_estimators=200, random_state=42)\n",
    "etc.fit(X_scaled, y_res)\n",
    "etc_importances = etc.feature_importances_\n",
    "\n",
    "# Calculate Mutual Information scores\n",
    "mi_scores = mutual_info_classif(X_scaled, y_res)\n",
    "\n",
    "# Select top 10 features using combined scores\n",
    "combined_scores = (etc_importances + mi_scores) / 2\n",
    "selected_indices = np.argsort(combined_scores)[-10:]\n",
    "X_selected = X_scaled[:, selected_indices]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_res, test_size=0.2, stratify=y_res, random_state=42)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, batch_first=True)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.classifier = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: (batch_size, features) -> (batch_size, features, d_model)\n",
    "        x = x.unsqueeze(1)  # Make it (batch_size, seq_len=1, d_model)\n",
    "        x = self.encoder(x)  # Transformer processes this\n",
    "        x = x[:, 0, :]  # Extract the first token (now shape is (batch_size, d_model))\n",
    "        return torch.sigmoid(self.classifier(x))  # Final output\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train.values, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize and train Transformer\n",
    "model = TransformerModel(input_dim=X_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Generate Transformer predictions\n",
    "with torch.no_grad():\n",
    "    test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    transformer_probs = model(test_tensor).numpy().flatten()\n",
    "\n",
    "# XGBoost Model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Stacking with Logistic Regression\n",
    "stacked_probs = np.column_stack([transformer_probs, xgb_probs])\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(stacked_probs, y_test)\n",
    "final_preds = meta_model.predict(stacked_probs)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, final_preds)\n",
    "precision = precision_score(y_test, final_preds)\n",
    "recall = recall_score(y_test, final_preds)\n",
    "f1 = f1_score(y_test, final_preds)\n",
    "auc = roc_auc_score(y_test, final_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWUdHekBY89L"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFU4FmbUWrQ9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
